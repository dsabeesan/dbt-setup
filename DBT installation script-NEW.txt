DBT INSTALLATION


NOTE:Before starting installation you should open the browser SYNC the end point verification and login the gcloud console with e.ID .. close all other browser, open only that gcloud console. 
Open CMD enter below command to create dbt folder

mkdir dbt
cd dbt
python -m venv venv
cd venv
cd Scripts
activate.bat
pip3 install dbt-bigquery

After installation of dbt-bigquery follow below commands


cd..
cd..

make sure that you are in dbt directory in CMD before going to next step


dbt init --profiles-dir dbtprj 
dbtsubprj2 
1
1
dmn01-rsksoi-bld-01-2017 

After giving project ID stop here and open C drive. GOTO Users - your empid folder - open dbt folder - there you can find "dbtprj" folder inside dbtprj paste project.yml file and come out paste dbt_project in dbt folder. 

Now back to your empid folder and create folder and name as ".dbt" and open the folder and paste the project.yml file

now come to CMD and continue with giving dataset name



dmn01_rsk_cog_raw
1
ENTER
2
 
gcloud config set project dmn01-rsksoi-bld-01-2017
gcloud auth application-default login 


dbt debug
 
After dbt debug everything should be green..... 




##########if your getting SSL verification disabled error, follow the this command: git config --global http.sslVerify false
dbt debug
#########################

Ingestion microservice
The ingestion microservice is structured into distinct packages, each playing a crucial role in the overall functionality. The 'src' package serves as the core, housing classes and mutations essential for execution. The 'bq' package facilitates interactions with BigQuery, while the 'driver' package oversees the entire ingestion process. 'Entity' encompasses classes defining the object structures for rules, messages, and metadata. The 'exception' package handles custom exceptions related to BigQuery. In the 'pubsub' package, classes are designed for interactions with PubSub. The 'service' package hosts methods for performing actions on files in Google Cloud Storage (GCS). The 'util' package offers fundamental functionalities for common use cases in the dataflow pipeline, and the 'validation' package provides utility classes for validation purposes. Additionally, the 'resources/<source system>' directory holds YAML files containing source-specific information for source tables. This comprehensive organization ensures a modular and efficient implementation of the ingestion microservice.

Staging Microservice
The Staging microservice is thoughtfully organized into distinct packages, with the 'src' package serving as the code's main body, defining classes and mutations crucial for execution. The 'bq' package facilitates seamless interactions with BigQuery, while the 'entity' package houses classes defining object structures for rules, messages, and metadata. In the 'pubsub' package, classes are dedicated to methods and sequencing, vital for pipeline builds. The 'util' package offers a range of utility classes, providing fundamental functionalities for common use cases in the dataflow pipeline. The 'validation' package not only contains utility classes for typical dataflow scenarios but also incorporates a class responsible for executing YAML files and publishing results to the validation metadata table. Furthermore, the 'resources/<source system>' package features folders corresponding to each source system, holding YAML files that encapsulate source table-specific information. This well-structured arrangement ensures the Staging microservice's efficiency, modularity, and seamless handling of data throughout the dataflow pipeline.

DBT
The DBT (Data Build Tool) repository for staging is composed of several integral components, each playing a unique role in the data transformation process. The 'macros' package contains reusable classes, facilitating the movement of data from raw to staging layers. In the 'models' package, SQL models are housed, responsible for transforming the data during its journey from raw to staging. The 'profiles' package contains essential files for connecting to Google Cloud Platform (GCP). The 'target' folder, generated after the initial DBT run, encompasses the executed SQL, methods, and sequencing vital for pipeline builds. The 'dbt_project.yml' file serves as a general setup for the DBT project, defining paths for models and profiles. Additional packages required by the DBT project are specified in 'packages.yml.' The 'project_run.sh' script acts as the run script, setting environment variables for local testing. This well-structured repository ensures a systematic and efficient data transformation process within the DBT framework for staging purposes.

Curation Microservice
The Curation Microservice is meticulously configured within a dedicated curation repository, where the core functionality is housed in the 'src' package. This package defines classes and mutations essential for execution, while references to a process curation message method in the Event Reader class from the Shared Library are crucial for seamless integration. Notably, the 'Pubsub' package encapsulates classes with methods and sequencing vital for pipeline builds. The 'ConfigProvider' class efficiently passes arguments from the 'application.yaml' file into configurations utilized by the 'CurationEventReader' class through Spring's '@Value' annotation. This latter class, in turn, orchestrates the creation and configuration of a PubSubInbound ChannelAdapter, a message channel, and a MessageHandler to execute the logic of processing curation messages from the shared library. The 'Resources' section includes a 'source' folder containing key YAML files, such as 'Application.yaml,' responsible for passing environment variables, 'Log4j2.xml,' a template file for log settings, and 'SchemaTableCheck.yaml,' a template outlining required staging tables before curation begins. Additionally, the 'Test' section is dedicated to unit tests for classes in the 'pubsub' package, ensuring adherence to the 80% code coverage mandated by Sonar Qube. This comprehensive structure ensures the Curation Microservice's effectiveness, maintainability, and compliance with testing standards.
